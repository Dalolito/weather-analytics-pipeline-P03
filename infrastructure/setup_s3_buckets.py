"""
Script completo para crear buckets S3 y configurar estructura local de datos
Mantiene compatibilidad con el proyecto original pero aÃ±ade organizaciÃ³n local mejorada
"""
import boto3
import json
import os
import shutil
from datetime import datetime
from botocore.exceptions import ClientError
from dotenv import load_dotenv

load_dotenv()

def create_s3_buckets():
    """Crear buckets S3 necesarios para el proyecto (funciÃ³n original)"""
    print("ğŸª£ Configurando buckets S3...")
    
    # Cliente S3
    s3_client = boto3.client('s3', region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1'))
    region = os.getenv('AWS_DEFAULT_REGION', 'us-east-1')
    
    # Nombres de buckets Ãºnicos
    timestamp = datetime.now().strftime('%Y%m%d')
    bucket_config = {
        'raw': f'weather-analytics-pipeline-raw-{timestamp}',
        'trusted': f'weather-analytics-pipeline-trusted-{timestamp}',
        'refined': f'weather-analytics-pipeline-refined-{timestamp}',
        'scripts': f'weather-analytics-pipeline-scripts-{timestamp}'
    }
    
    created_buckets = {}
    
    for bucket_type, bucket_name in bucket_config.items():
        try:
            print(f"ğŸ“ Creando bucket {bucket_type}: {bucket_name}")
            
            # Crear bucket
            if region == 'us-east-1':
                # us-east-1 no necesita LocationConstraint
                s3_client.create_bucket(Bucket=bucket_name)
            else:
                s3_client.create_bucket(
                    Bucket=bucket_name,
                    CreateBucketConfiguration={'LocationConstraint': region}
                )
            
            # Configurar versionado
            s3_client.put_bucket_versioning(
                Bucket=bucket_name,
                VersioningConfiguration={'Status': 'Enabled'}
            )
            
            # Configurar encriptaciÃ³n
            s3_client.put_bucket_encryption(
                Bucket=bucket_name,
                ServerSideEncryptionConfiguration={
                    'Rules': [
                        {
                            'ApplyServerSideEncryptionByDefault': {
                                'SSEAlgorithm': 'AES256'
                            }
                        }
                    ]
                }
            )
            
            # Crear estructura de folders para bucket raw
            if bucket_type == 'raw':
                folders = [
                    'weather-api/',
                    'weather-historical/',
                    'database/'
                ]
                for folder in folders:
                    s3_client.put_object(
                        Bucket=bucket_name,
                        Key=folder,
                        Body=''
                    )
                    
            # Crear estructura para bucket trusted
            elif bucket_type == 'trusted':
                folders = [
                    'weather_data/',
                    'weather_stations/',
                    'climate_events/',
                    'integrated_weather_data/'
                ]
                for folder in folders:
                    s3_client.put_object(
                        Bucket=bucket_name,
                        Key=folder,
                        Body=''
                    )
                    
            # Crear estructura para bucket refined
            elif bucket_type == 'refined':
                folders = [
                    'temperature_trends_monthly/',
                    'extreme_weather_events/',
                    'annual_weather_summary/',
                    'ml_model_metrics/',
                    'temperature_predictions/',
                    'weather_forecasts/',
                    'athena-results/',
                    'validation_reports/'
                ]
                for folder in folders:
                    s3_client.put_object(
                        Bucket=bucket_name,
                        Key=folder,
                        Body=''
                    )
                    
            # Crear estructura para bucket scripts
            elif bucket_type == 'scripts':
                folders = [
                    'spark-jobs/',
                    'bootstrap/',
                    'logs/'
                ]
                for folder in folders:
                    s3_client.put_object(
                        Bucket=bucket_name,
                        Key=folder,
                        Body=''
                    )
            
            created_buckets[bucket_type] = bucket_name
            print(f"âœ… Bucket {bucket_type} creado exitosamente")
            
        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code == 'BucketAlreadyOwnedByYou':
                print(f"â„¹ï¸ Bucket {bucket_name} ya existe y es tuyo")
                created_buckets[bucket_type] = bucket_name
            elif error_code == 'BucketAlreadyExists':
                print(f"âŒ Bucket {bucket_name} ya existe (propiedad de otra cuenta)")
                # Intentar con un nombre mÃ¡s Ãºnico
                unique_bucket = f"{bucket_name}-{os.getenv('USER', 'user')}"
                try:
                    if region == 'us-east-1':
                        s3_client.create_bucket(Bucket=unique_bucket)
                    else:
                        s3_client.create_bucket(
                            Bucket=unique_bucket,
                            CreateBucketConfiguration={'LocationConstraint': region}
                        )
                    created_buckets[bucket_type] = unique_bucket
                    print(f"âœ… Bucket alternativo creado: {unique_bucket}")
                except Exception as e2:
                    print(f"âŒ Error creando bucket alternativo: {e2}")
            else:
                print(f"âŒ Error creando bucket {bucket_name}: {e}")
    
    return created_buckets

def setup_local_structure():
    """Crear estructura de carpetas locales con organizaciÃ³n por fecha"""
    print("\nğŸ“ Configurando estructura local de datos...")
    
    # Configurar rutas
    scripts_folder = "scripts"
    timestamp = datetime.now().strftime('%Y%m%d')
    date_folder = f"weather-analytics-pipeline-{timestamp}"
    local_data_path = os.path.join(scripts_folder, date_folder)
    
    # Crear carpeta principal scripts si no existe
    if not os.path.exists(scripts_folder):
        os.makedirs(scripts_folder)
        print(f"âœ… Carpeta '{scripts_folder}' creada")
    
    # Crear subcarpeta con fecha
    if not os.path.exists(local_data_path):
        os.makedirs(local_data_path)
        print(f"âœ… Carpeta de fecha '{local_data_path}' creada")
    
    # Crear subcarpetas para cada tipo de bucket S3
    bucket_folders = {
        'raw': [
            'weather-api',
            'weather-historical', 
            'database',
            'logs'
        ],
        'trusted': [
            'weather_data',
            'weather_stations',
            'climate_events',
            'integrated_weather_data'
        ],
        'refined': [
            'temperature_trends_monthly',
            'extreme_weather_events',
            'annual_weather_summary',
            'ml_model_metrics',
            'temperature_predictions',
            'weather_forecasts',
            'athena-results',
            'validation_reports'
        ],
        'scripts': [
            'spark-jobs',
            'bootstrap',
            'logs'
        ]
    }
    
    for bucket_type, subfolders in bucket_folders.items():
        bucket_path = os.path.join(local_data_path, bucket_type)
        if not os.path.exists(bucket_path):
            os.makedirs(bucket_path)
            print(f"   ğŸ“‚ Carpeta '{bucket_type}' creada")
            
            # Crear subcarpetas especÃ­ficas
            for subfolder in subfolders:
                subfolder_path = os.path.join(bucket_path, subfolder)
                if not os.path.exists(subfolder_path):
                    os.makedirs(subfolder_path)
                    print(f"      ğŸ“„ Subcarpeta '{subfolder}' creada")
    
    # Crear archivo de metadatos
    metadata = {
        "creation_date": datetime.now().strftime('%Y-%m-%d'),
        "timestamp": timestamp,
        "project": "weather-analytics-pipeline",
        "description": "Datos de ingesta y procesamiento para anÃ¡lisis meteorolÃ³gico",
        "bucket_types": list(bucket_folders.keys()),
        "s3_buckets": {},
        "local_path": local_data_path,
        "structure": bucket_folders
    }
    
    metadata_path = os.path.join(local_data_path, 'metadata.json')
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"âœ… Metadatos guardados en {metadata_path}")
    
    # Crear archivo README con instrucciones
    readme_content = f"""# Weather Analytics Pipeline - {timestamp}

## Estructura de Datos

Esta carpeta contiene la estructura local de datos para el proyecto Weather Analytics Pipeline.

### Fecha de creaciÃ³n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

### Estructura de carpetas:

```
{date_folder}/
â”œâ”€â”€ raw/                          # Datos crudos
â”‚   â”œâ”€â”€ weather-api/             # Datos de API OpenMeteo
â”‚   â”œâ”€â”€ weather-historical/      # Datos histÃ³ricos
â”‚   â”œâ”€â”€ database/               # Datos de base de datos
â”‚   â””â”€â”€ logs/                   # Logs de ingesta
â”œâ”€â”€ trusted/                     # Datos procesados y limpios
â”‚   â”œâ”€â”€ weather_data/           # Datos meteorolÃ³gicos procesados
â”‚   â”œâ”€â”€ weather_stations/       # Estaciones meteorolÃ³gicas
â”‚   â”œâ”€â”€ climate_events/         # Eventos climÃ¡ticos
â”‚   â””â”€â”€ integrated_weather_data/ # Datos integrados
â”œâ”€â”€ refined/                     # Datos analÃ­ticos y resultados
â”‚   â”œâ”€â”€ temperature_trends_monthly/
â”‚   â”œâ”€â”€ extreme_weather_events/
â”‚   â”œâ”€â”€ annual_weather_summary/
â”‚   â”œâ”€â”€ ml_model_metrics/
â”‚   â”œâ”€â”€ temperature_predictions/
â”‚   â”œâ”€â”€ weather_forecasts/
â”‚   â”œâ”€â”€ athena-results/
â”‚   â””â”€â”€ validation_reports/
â”œâ”€â”€ scripts/                     # Scripts y configuraciones
â”‚   â”œâ”€â”€ spark-jobs/             # Jobs de Spark
â”‚   â”œâ”€â”€ bootstrap/              # Scripts de bootstrap
â”‚   â””â”€â”€ logs/                   # Logs de procesamiento
â””â”€â”€ metadata.json               # Metadatos del proyecto
```

### Uso:

1. **Ingesta de datos**: Los scripts de ingesta guardarÃ¡n automÃ¡ticamente los datos tanto en S3 como en las carpetas locales correspondientes.

2. **Procesamiento**: Los datos procesados se almacenarÃ¡n en las carpetas `trusted/` y `refined/`.

3. **Logs**: Todos los logs se guardan en las carpetas `logs/` respectivas.

### SincronizaciÃ³n con S3:

- Los datos se almacenan simultÃ¡neamente en S3 y localmente
- Los buckets S3 estÃ¡n configurados en `config/buckets.json`
- La sincronizaciÃ³n es automÃ¡tica durante los procesos de ingesta

### Archivos importantes:

- `metadata.json`: Contiene informaciÃ³n del proyecto y configuraciÃ³n
- `config/buckets.json`: ConfiguraciÃ³n de buckets S3 (en directorio raÃ­z)

Para mÃ¡s informaciÃ³n, consulta la documentaciÃ³n del proyecto.
"""
    
    readme_path = os.path.join(local_data_path, 'README.md')
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"âœ… README creado en {readme_path}")
    
    return local_data_path, metadata

def update_metadata_with_buckets(local_data_path, bucket_config):
    """Actualizar archivo de metadatos con informaciÃ³n de buckets S3"""
    metadata_path = os.path.join(local_data_path, 'metadata.json')
    
    try:
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        metadata['s3_buckets'] = bucket_config
        metadata['setup_completed'] = datetime.now().isoformat()
        
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
            
        print(f"âœ… Metadatos actualizados con informaciÃ³n de S3")
        
    except Exception as e:
        print(f"âš ï¸ Error actualizando metadatos: {e}")

def verify_buckets(bucket_config):
    """Verificar que los buckets estÃ©n accesibles (funciÃ³n original)"""
    print("\nğŸ” Verificando acceso a buckets...")
    
    s3_client = boto3.client('s3', region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1'))
    
    all_accessible = True
    for bucket_type, bucket_name in bucket_config.items():
        try:
            # Intentar listar objetos
            response = s3_client.list_objects_v2(Bucket=bucket_name, MaxKeys=1)
            print(f"âœ… {bucket_type}: {bucket_name} - Accesible")
        except Exception as e:
            print(f"âŒ {bucket_type}: {bucket_name} - Error: {e}")
            all_accessible = False
    
    return all_accessible

def main():
    """FunciÃ³n principal mejorada que mantiene compatibilidad"""
    print("ğŸš€ CONFIGURACIÃ“N COMPLETA DE BUCKETS S3 Y ESTRUCTURA LOCAL")
    print("="*70)
    
    try:
        # Verificar credenciales AWS
        sts = boto3.client('sts', region_name=os.getenv('AWS_DEFAULT_REGION', 'us-east-1'))
        identity = sts.get_caller_identity()
        print(f"ğŸ”‘ Conectado como: {identity.get('Arn', 'Unknown')}")
        
        # Paso 1: Configurar estructura local
        print(f"\nğŸ“‚ Paso 1: Configurando estructura local...")
        local_data_path, metadata = setup_local_structure()
        
        # Paso 2: Crear buckets S3 (funciÃ³n original)
        print(f"\nâ˜ï¸ Paso 2: Creando buckets S3...")
        created_buckets = create_s3_buckets()
        
        # Paso 3: Verificar acceso (funciÃ³n original)
        print(f"\nğŸ” Paso 3: Verificando acceso...")
        buckets_ok = verify_buckets(created_buckets)
        
        # Paso 4: Guardar configuraciÃ³n
        print(f"\nğŸ’¾ Paso 4: Guardando configuraciÃ³n...")
        
        # Guardar configuraciÃ³n de buckets (mantiene compatibilidad)
        os.makedirs('config', exist_ok=True)
        with open('config/buckets.json', 'w') as f:
            json.dump(created_buckets, f, indent=2)
        
        print(f"âœ… ConfiguraciÃ³n guardada en config/buckets.json")
        
        # Actualizar metadatos locales
        update_metadata_with_buckets(local_data_path, created_buckets)
        
        # Mostrar resumen
        if buckets_ok:
            print("\nğŸ‰ Â¡ConfiguraciÃ³n completada exitosamente!")
            print("\n" + "="*50)
            print("ğŸ“Š RESUMEN DE CONFIGURACIÃ“N")
            print("="*50)
            
            print(f"\nâ˜ï¸ Buckets S3 creados:")
            for bucket_type, bucket_name in created_buckets.items():
                print(f"   â€¢ {bucket_type}: {bucket_name}")
            
            print(f"\nğŸ“ Estructura local creada:")
            print(f"   ğŸ“ UbicaciÃ³n: {local_data_path}")
            print(f"   ğŸ“‚ Carpetas: raw, trusted, refined, scripts")
            print(f"   ğŸ“„ Archivos: metadata.json, README.md")
            
            print(f"\nâš™ï¸ Archivos de configuraciÃ³n:")
            print(f"   â€¢ config/buckets.json (compatibilidad con proyecto original)")
            print(f"   â€¢ {local_data_path}/metadata.json (informaciÃ³n extendida)")
            
            print(f"\nğŸ¯ PrÃ³ximos pasos:")
            print(f"   1. Ejecutar ingesta: python ingesta/fetch_openmeteo_api.py")
            print(f"   2. Los datos se guardarÃ¡n automÃ¡ticamente en:")
            print(f"      - S3: buckets configurados")
            print(f"      - Local: {local_data_path}")
            print(f"   3. Procesar datos: python run_project.py --step ingest-data")
            
        else:
            print("\nâš ï¸ ConfiguraciÃ³n completada con algunos problemas de acceso a S3")
            print("ğŸ’¡ Los datos se pueden seguir guardando localmente")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Error en configuraciÃ³n: {e}")
        print("ğŸ’¡ AsegÃºrate de que:")
        print("   1. AWS_DEFAULT_REGION estÃ© configurado en .env")
        print("   2. Tengas credenciales AWS vÃ¡lidas")
        print("   3. Tengas permisos para crear buckets S3")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)