# Weather Analytics Big Data Project

## AutomatizaciÃ³n del proceso de Captura, Ingesta, Procesamiento y Salida de datos meteorolÃ³gicos

**Universidad EAFIT**  
**ST0263: TÃ³picos Especiales en TelemÃ¡tica, 2025-1**  
**Trabajo 3 - Arquitectura Batch para Big Data**

---

## Estudiante(s):
| Nombre | Correo |
|--------|-------------|
| David Lopera LondoÃ±o | dloperal2@eafit.edu.co |
| Camilo Monsalve Montes | cmonsalvem@eafit.edu.co |
| Juan Diego AcuÃ±a Giraldo | jdacunag@eafit.edu.co |

## Profesor:
| Profesor | Correo |
|----------|-------------|
| Edwin Nelson Montoya MÃºnera | emontoya@eafit.edu.co |

## VÃ­deo de la SustentaciÃ³n

## ğŸ“‹ Tabla de Contenidos

1. [DescripciÃ³n del Proyecto](#descripciÃ³n-del-proyecto)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [TecnologÃ­as Utilizadas](#tecnologÃ­as-utilizadas)
4. [Estructura del Proyecto](#estructura-del-proyecto)
5. [Fuentes de Datos](#fuentes-de-datos)
6. [ImplementaciÃ³n](#implementaciÃ³n)
7. [Casos de Uso y Resultados](#casos-de-uso-y-resultados)
8. [GuÃ­a de InstalaciÃ³n](#guÃ­a-de-instalaciÃ³n)
9. [ValidaciÃ³n y Pruebas](#validaciÃ³n-y-pruebas)
10. [API y Consultas](#api-y-consultas)
11. [Monitoreo y Logs](#monitoreo-y-logs)
12. [VisualizaciÃ³n con Colab](#visualizaciÃ³n-con-colab)
13. [Conclusiones](#conclusiones)
14. [Referencias y Recursos](#referencias-y-recursos)

---
## 1. ğŸ¯ DescripciÃ³n del Proyecto

Este proyecto implementa una **arquitectura batch completa de Big Data** para anÃ¡lisis meteorolÃ³gico automatizado usando tecnologÃ­as AWS y Apache Spark. El sistema automatiza el proceso completo desde la captura de datos hasta los modelos predictivos y consultas SQL, cumpliendo con todos los requerimientos de una soluciÃ³n de ingenierÃ­a de datos real.

### Objetivos Principales

- **AutomatizaciÃ³n completa** del ciclo de vida de datos meteorolÃ³gicos
- **Procesamiento masivo** de datos histÃ³ricos y en tiempo real
- **AnÃ¡lisis descriptivo y predictivo** con Machine Learning
- **Acceso a datos** mediante consultas SQL y APIs REST
- **Escalabilidad** y tolerancia a fallos en la nube

### Objetivos EspecÃ­ficos
âœ… Captura automÃ¡tica de datos meteorolÃ³gicos desde API OpenMeteo y base de datos MySQL

âœ… Ingesta automatizada hacia buckets S3 organizados por zonas (Raw, Trusted, Refined)

âœ… Procesamiento ETL con Apache Spark en clÃºster EMR

âœ… AnÃ¡lisis descriptivo y modelos de machine learning

âœ… Consultas SQL mediante Amazon Athena

âœ… API REST para acceso a resultados


### Problema Resuelto

AnÃ¡lisis meteorolÃ³gico avanzado para 5 ciudades colombianas principales (BogotÃ¡, MedellÃ­n, Cali, Cartagena, Barranquilla) con capacidad de:
- Detectar eventos climÃ¡ticos extremos
- Predecir tendencias de temperatura y precipitaciÃ³n
- Generar pronÃ³sticos basados en datos histÃ³ricos
- Proporcionar APIs para aplicaciones externas

## Aspectos NO cumplidos o desarrollados
Tuvimos limitaciones con:

* **AWS Academy:** Permisos limitados para algunos servicios avanzados
* **API Gateway:** ConfiguraciÃ³n manual requerida (instrucciones proporcionadas)
* **Machine Learning:** ImplementaciÃ³n simplificada debido a restricciones de librerÃ­as en EMR

---

## 2. ğŸ—ï¸ Arquitectura del Sistema

### Pipeline de Datos (Data Lake Architecture)

```
[Fuentes de Datos] â†’ [Ingesta] â†’ [Raw Zone] â†’ [ETL] â†’ [Trusted Zone] â†’ [Analytics/ML] â†’ [Refined Zone] â†’ [Athena/APIs]
```

### Componentes ArquitectÃ³nicos

#### 1. **Zona Raw (Datos Crudos)**
- **Almacenamiento**: Amazon S3
- **Contenido**: Datos sin procesar de APIs y base de datos
- **Estructura**: Particionado por fuente, ciudad y fecha

![image](https://github.com/user-attachments/assets/9aef5c29-7a29-44c4-9c6f-7124664c0598)
*Captura de la estructura de carpetas en S3 Raw Zone*

#### 2. **Zona Trusted (Datos Procesados)**
- **Procesamiento**: Apache Spark en Amazon EMR
- **Transformaciones**: Limpieza, estandarizaciÃ³n, integraciÃ³n
- **Formato**: Apache Parquet con compresiÃ³n

![image](https://github.com/user-attachments/assets/745a9240-4c3f-49ff-9149-4d7f2592301f)
*Captura de archivos Parquet en S3 Trusted Zone*

#### 3. **Zona Refined (Datos AnalÃ­ticos)**
- **Contenido**: Resultados de anÃ¡lisis y modelos ML
- **Formato**: Parquet optimizado para consultas
- **Uso**: Athena, APIs y dashboards

![image](https://github.com/user-attachments/assets/4c355e45-df8a-4f8b-8b40-123014287672)

*Captura de resultados analÃ­ticos en S3 Refined Zone*

### Arquitectura Cloud (AWS)
![image](https://github.com/user-attachments/assets/d0c31155-6ef9-4e13-bc56-0892a6fda5cb)


```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   Amazon EMR    â”‚    â”‚   Amazon S3     â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ OpenMeteo API â”‚â”€â”€â”€â–¶â”‚ â€¢ Spark Cluster â”‚â”€â”€â”€â–¶â”‚ â€¢ Data Lake     â”‚
â”‚ â€¢ RDS MySQL     â”‚    â”‚ â€¢ Auto-scaling  â”‚    â”‚ â€¢ 3 Zones       â”‚
â”‚ â€¢ Historical    â”‚    â”‚ â€¢ Managed       â”‚    â”‚ â€¢ Partitioned   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                              â”‚
         â–¼                                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AWS Lambda    â”‚                            â”‚  Amazon Athena  â”‚
â”‚                 â”‚                            â”‚                 â”‚
â”‚ â€¢ API Gateway   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â€¢ SQL Engine    â”‚
â”‚ â€¢ REST APIs     â”‚                            â”‚ â€¢ Serverless    â”‚
â”‚ â€¢ Serverless    â”‚                            â”‚ â€¢ Query Results â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

![Captura de pantalla 2025-06-02 190228](https://github.com/user-attachments/assets/f29d5843-62ab-4fb0-88a1-f0d38c1c0bca)
*Captura de los servicios AWS utilizados en la consola*

---

## 3. ğŸ’» TecnologÃ­as Utilizadas

### Servicios AWS

| Servicio | PropÃ³sito | ConfiguraciÃ³n |
|----------|-----------|---------------|
| **Amazon S3** | Data Lake storage | 4 buckets (Raw, Trusted, Refined, Scripts) |
| **Amazon EMR** | Procesamiento Spark | ClÃºster m5.xlarge + 2 m5.large |
| **Amazon RDS** | Base de datos relacional | MySQL 8.0 |
| **Amazon Athena** | Motor de consultas SQL | Serverless |
| **AWS Lambda** | API serverless | Python 3.9 runtime |
| **API Gateway** | REST API management | HTTP APIs |

![image](https://github.com/user-attachments/assets/98b02165-c5c6-4697-8b96-b7bc35acfb3d)
*Captura de la configuraciÃ³n del clÃºster EMR*

### Stack TecnolÃ³gico

- **Lenguaje**: Python 3.8+
- **Big Data**: Apache Spark 3.4
- **Machine Learning**: Spark MLlib
- **Formato de Datos**: Apache Parquet
- **OrquestaciÃ³n**: Python scripts + AWS services
- **APIs**: REST con JSON
- **Testing**: pytest + moto

---

## 4. ğŸ“ Estructura del Proyecto

```
weather-analytics-bigdata/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # DocumentaciÃ³n principal
â”œâ”€â”€ ğŸ“„ requirements.txt             # Dependencias Python
â”œâ”€â”€ ğŸ“„ .gitignore                   # Archivos a ignorar
â”œâ”€â”€ ğŸ“„ run_project.py              # Script principal de ejecuciÃ³n
â”‚
â”œâ”€â”€ ğŸ“ config/                      # Configuraciones
â”‚   â”œâ”€â”€ config.py                  # Configuraciones generales
â”‚   â””â”€â”€ buckets.json              # ConfiguraciÃ³n de buckets S3
â”‚
â”œâ”€â”€ ğŸ“ infrastructure/              # Scripts de infraestructura
â”‚   â”œâ”€â”€ setup_s3_buckets.py       # CreaciÃ³n de buckets S3
â”‚   â”œâ”€â”€ setup_database.py         # ConfiguraciÃ³n de RDS
â”‚   â”œâ”€â”€ setup_emr_cluster.py      # CreaciÃ³n de clÃºster EMR
â”‚   â””â”€â”€ fix_database_connection.py # DiagnÃ³stico de RDS
â”‚
â”œâ”€â”€ ğŸ“ ingesta/                     # Scripts de ingesta
â”‚   â”œâ”€â”€ fetch_openmeteo_api.py     # Ingesta desde API
â”‚   â””â”€â”€ fetch_database.py         # Ingesta desde RDS
â”‚
â”œâ”€â”€ ğŸ“ spark_jobs/                  # Jobs de Spark
â”‚   â”œâ”€â”€ etl_weather_data.py        # ETL y limpieza
â”‚   â”œâ”€â”€ analytics_weather_trends.py # AnÃ¡lisis descriptivo
â”‚   â”œâ”€â”€ ml_weather_prediction.py   # Machine Learning
â”‚   â””â”€â”€ test_spark_jobs.py         # Tests de Spark
â”‚
â”œâ”€â”€ ğŸ“ automation/                  # OrquestaciÃ³n
â”‚   â””â”€â”€ orchestrator.py           # Coordinador de pipeline
â”‚
â”œâ”€â”€ ğŸ“ applications/                # Aplicaciones finales
â”‚   â”œâ”€â”€ setup_athena.py           # ConfiguraciÃ³n Athena
â”‚   â””â”€â”€ weather_api.py            # Lambda para API
|
â”œâ”€â”€ ğŸ“ visualizations/
â”‚   â”œâ”€â”€ notebooks/ # Carpeta con los notebooks para usar en Colab 
â”‚         â””â”€â”€ Colab1/ #Carpe # Carpeta que contiene los archivos .json y .csv para adjuntar a Colab 
â”‚
â””â”€â”€ ğŸ“ tests/                       # Tests y validaciones
    â”œâ”€â”€ test_connections.py        # Tests de conectividad
    â”œâ”€â”€ validate_pipeline.py       # ValidaciÃ³n completa
    â””â”€â”€ validate_pipeline_improved.py # ValidaciÃ³n mejorada
```

---

## 5. ğŸŒ Fuentes de Datos

### 1. OpenMeteo API (Datos en Tiempo Real)

**Fuente**: https://api.open-meteo.com/v1/forecast

```python
# Ciudades colombianas analizadas
CITIES = {
    'bogota': {'lat': 4.6097, 'lon': -74.0817, 'name': 'BogotÃ¡'},
    'medellin': {'lat': 6.2518, 'lon': -75.5636, 'name': 'MedellÃ­n'},
    'cali': {'lat': 3.4516, 'lon': -76.5320, 'name': 'Cali'},
    'cartagena': {'lat': 10.3910, 'lon': -75.4794, 'name': 'Cartagena'},
    'barranquilla': {'lat': 10.9639, 'lon': -74.7964, 'name': 'Barranquilla'}
}
```

**Variables extraÃ­das:**
- Temperatura mÃ¡xima, mÃ­nima y promedio
- PrecipitaciÃ³n acumulada
- Velocidad del viento
- Humedad relativa
- PresiÃ³n atmosfÃ©rica

![image](https://github.com/user-attachments/assets/7ec8fc66-1cab-4324-98d4-9c3c7f38fac1)

*Captura de respuesta JSON de la API OpenMeteo*

### 2. Base de Datos Relacional (RDS MySQL)

**Esquema de datos:**

```sql
-- Estaciones meteorolÃ³gicas
CREATE TABLE weather_stations (
    station_id VARCHAR(50) PRIMARY KEY,
    station_name VARCHAR(200) NOT NULL,
    latitude DECIMAL(10, 6) NOT NULL,
    longitude DECIMAL(10, 6) NOT NULL,
    elevation INT,
    city VARCHAR(100),
    department VARCHAR(100),
    installation_date DATE,
    station_type VARCHAR(50),
    status ENUM('active', 'inactive', 'maintenance')
);

-- Eventos climÃ¡ticos histÃ³ricos
CREATE TABLE climate_events (
    event_id INT AUTO_INCREMENT PRIMARY KEY,
    station_id VARCHAR(50),
    event_type ENUM('storm', 'drought', 'flood', 'heatwave', 'coldwave'),
    event_date DATE NOT NULL,
    severity ENUM('low', 'medium', 'high', 'extreme'),
    description TEXT,
    impact_area VARCHAR(200),
    economic_impact DECIMAL(15, 2),
    people_affected INT
);
```

![image](https://github.com/user-attachments/assets/48106883-ddea-4eab-8d77-af59b68f01b9)

*Captura de las tablas en RDS MySQL desde la consola AWS*

### Estructura de Datos en S3

#### Raw Zone
```
s3://weather-analytics-pipeline-raw-20250527/
â”œâ”€â”€ weather-api/
â”‚   â”œâ”€â”€ bogota/2025/05/30/14/weather_data.json
â”‚   â”œâ”€â”€ medellin/2025/05/30/14/weather_data.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ weather_stations/2025/05/30/14/weather_stations_data.json
â”‚   â”œâ”€â”€ climate_events/2025/05/30/14/climate_events_data.json
â”‚   â””â”€â”€ weather_thresholds/2025/05/30/14/weather_thresholds_data.json
```


![image](https://github.com/user-attachments/assets/ed853508-1b4c-4108-84b2-a23efa16a94b)
![image](https://github.com/user-attachments/assets/9ee7669b-df59-4b76-ac81-dcb079f2e524)

*Captura de un archivo JSON de datos crudos en S3*

---

## 6. âš™ï¸ ImplementaciÃ³n

### 1. Proceso de Ingesta Automatizada

#### Ingesta desde API OpenMeteo

```python
class OpenMeteoAPIIngester:
    def fetch_current_weather(self, city_info):
        """Obtener datos meteorolÃ³gicos actuales"""
        params = {
            'latitude': city_info['lat'],
            'longitude': city_info['lon'],
            'current_weather': 'true',
            'daily': ','.join(WEATHER_VARIABLES),
            'timezone': 'America/Bogota',
            'forecast_days': 7
        }
        
        response = requests.get(f"{OPENMETEO_BASE_URL}/forecast", params=params)
        # Procesamiento y subida a S3...
```

![image](https://github.com/user-attachments/assets/61c97fb7-3edc-4f8e-a02a-436892d6ad58)

*Captura de logs de ingesta en CloudWatch*

#### Ingesta desde Base de Datos

```python
class DatabaseIngester:
    def extract_weather_stations(self):
        """Extraer datos de estaciones meteorolÃ³gicas"""
        query = """
        SELECT station_id, station_name, latitude, longitude, 
               elevation, city, department, country, 
               installation_date, station_type, status
        FROM weather_stations 
        WHERE status = 'active'
        """
        df = pd.read_sql(query, connection)
        return df
```

### 2. Procesamiento ETL con Spark

#### Limpieza y TransformaciÃ³n

```python
def process_weather_api_data(spark, raw_bucket, trusted_bucket):
    """Procesar datos de API meteorolÃ³gica"""
    # Leer datos crudos
    weather_df = spark.read.json(f"s3a://{raw_bucket}/weather-api/*/weather_data.json")
    
    # Limpiar y transformar
    weather_clean = weather_df.select(
        col("city_info.name").alias("city_name"),
        col("daily.temperature_2m_max").alias("temp_max"),
        col("daily.temperature_2m_min").alias("temp_min"),
        # ... mÃ¡s transformaciones
    ).filter(col("city_name").isNotNull())
    
    # Calcular temperatura promedio
    weather_clean = weather_clean.withColumn(
        "temp_avg", (col("temp_max") + col("temp_min")) / 2
    )
```


#### IntegraciÃ³n de Datos

```python
def create_integrated_dataset(spark, trusted_bucket):
    """Crear dataset integrado combinando API y BD"""
    weather_df = spark.read.parquet(f"s3a://{trusted_bucket}/weather_data/")
    stations_df = spark.read.parquet(f"s3a://{trusted_bucket}/weather_stations/")
    
    # Join por coordenadas geogrÃ¡ficas
    integrated_df = weather_df.join(
        stations_df,
        (weather_df.city_name == stations_df.city) |
        (abs(weather_df.latitude - stations_df.latitude) < 0.1),
        "left"
    )
```

### 3. AnÃ¡lisis Descriptivo y Predictivo

#### AnÃ¡lisis de Tendencias

```python
def analyze_temperature_trends(spark, trusted_bucket, refined_bucket):
    """Analizar tendencias de temperatura"""
    weather_df = spark.read.parquet(f"s3a://{trusted_bucket}/integrated_weather_data/")
    
    # AnÃ¡lisis mensual por ciudad
    monthly_trends = weather_df.groupBy("city_name", "year", "month") \
        .agg(
            avg("temp_avg").alias("avg_temperature"),
            max("temp_max").alias("max_temperature"),
            min("temp_min").alias("min_temperature"),
            avg("precipitation").alias("avg_precipitation")
        )
```


#### DetecciÃ³n de Eventos Extremos

```python
def analyze_extreme_weather(spark, trusted_bucket, refined_bucket):
    """Analizar eventos meteorolÃ³gicos extremos"""
    extreme_events = weather_df.filter(
        (col("temp_max") > 35) |      # Calor extremo
        (col("temp_min") < 0) |       # FrÃ­o extremo
        (col("precipitation") > 50) | # Lluvia intensa
        (col("wind_speed") > 60)      # Vientos fuertes
    ).withColumn("event_type", 
        when(col("temp_max") > 35, "extreme_heat")
        .when(col("temp_min") < 0, "extreme_cold")
        # ... mÃ¡s clasificaciones
    )
```

### 4. Machine Learning con Spark MLlib

#### Modelo de PredicciÃ³n de Temperatura

```python
def train_temperature_prediction_model(spark, feature_df, refined_bucket):
    """Entrenar modelo de predicciÃ³n de temperatura"""
    # CaracterÃ­sticas para el modelo
    feature_cols = [
        "temp_lag1", "temp_lag2", "precipitation", "humidity", 
        "wind_speed", "elevation", "day_of_year", "season"
    ]
    
    # Pipeline de ML
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
    rf = RandomForestRegressor(featuresCol="scaled_features", labelCol="temp_avg")
    
    pipeline = Pipeline(stages=[assembler, scaler, rf])
    model = pipeline.fit(train_data)
```

![image](https://github.com/user-attachments/assets/d84deaec-24b0-45ae-a06f-0809fe4548bf)

*Captura de mÃ©tricas de modelos ML guardadas en S3*

---

## 7. ğŸ“Š Casos de Uso y Resultados

### Datos Procesados

- **35+ archivos** de datos meteorolÃ³gicos procesados
- **5 ciudades colombianas**: BogotÃ¡, MedellÃ­n, Cali, Cartagena, Barranquilla
- **7 dÃ­as** de pronÃ³sticos por ciudad
- **Eventos extremos** detectados y clasificados
- **Modelos ML** entrenados con mÃ©tricas de evaluaciÃ³n

### AnÃ¡lisis Realizados

#### 1. Tendencias de Temperatura por Ciudad

```sql
SELECT city_name, AVG(temp_avg) as avg_temp 
FROM weather_analytics_db.integrated_weather_data 
GROUP BY city_name 
ORDER BY avg_temp DESC;
```

**Resultados:**
- Cartagena: 28.5Â°C (clima mÃ¡s cÃ¡lido)
- Barranquilla: 27.8Â°C
- Cali: 24.2Â°C
- MedellÃ­n: 22.1Â°C
- BogotÃ¡: 15.7Â°C (clima mÃ¡s frÃ­o)


#### 2. Eventos MeteorolÃ³gicos Extremos

```sql
SELECT city_name, event_type, SUM(event_count) as total_events 
FROM weather_analytics_db.extreme_weather_events 
GROUP BY city_name, event_type 
ORDER BY total_events DESC;
```

![image](https://github.com/user-attachments/assets/112c95fe-0d59-494b-9951-98d8550e42d6)

*Captura de query de  anÃ¡lisis de eventos extremos*

#### 3. PrecisiÃ³n de Modelos ML

| Modelo | RMSE | MÃ©trica |
|--------|------|---------|
| PredicciÃ³n Temperatura | 2.1Â°C | Muy buena precisiÃ³n |
| PredicciÃ³n PrecipitaciÃ³n | 8.5mm | Aceptable |


---

## 8. ğŸš€ GuÃ­a de InstalaciÃ³n

### Prerequisitos

1. **AWS Academy Account** con acceso a S3, RDS, EMR, Athena
2. **Python 3.8+** con pip instalado
3. **Credenciales AWS** configuradas

### InstalaciÃ³n Paso a Paso

#### 1. Clonar y Configurar

```bash
git clone <repositorio>
cd weather-analytics-bigdata
python -m venv venv
source venv/bin/activate  # Linux/Mac
pip install -r requirements.txt
```

#### 2. Configurar Variables de Entorno

```bash
# Crear archivo .env
cat > .env << EOF
AWS_DEFAULT_REGION=us-east-1
AWS_ACCESS_KEY_ID=tu_access_key
AWS_SECRET_ACCESS_KEY=tu_secret_key

# Opcional: Base de datos
DB_HOST=tu-rds-endpoint.us-east-1.rds.amazonaws.com
DB_USER=admin
DB_PASSWORD=tu_password
DB_NAME=weather_data
EOF
```

#### 3. Verificar Conexiones

```bash
python tests/test_connections.py
```

![Image](https://github.com/user-attachments/assets/873d6879-1727-4a87-95f3-f45f4ac42d40)
*Captura de tests de conectividad exitosos*

#### 4. Configurar Infraestructura AWS

```bash
# Crear buckets S3
python infrastructure/setup_s3_buckets.py

# Crear clÃºster EMR
python infrastructure/setup_emr_cluster.py
```

![image](https://github.com/user-attachments/assets/ba8f133e-a795-4b03-80c5-94460a9687b4)

![Image](https://github.com/user-attachments/assets/711fd955-31da-4bfd-a505-81c8468e2309)

![image](https://github.com/user-attachments/assets/d5302951-596d-4b2a-b05f-e9a1d2971748)

*Capturas de buckets S3 creados exitosamente*

#### 5. Ejecutar Pipeline Completo

```bash
# Obtener ID del clÃºster EMR desde la consola AWS
python run_project.py --step full-pipeline --cluster-id j-XXXXXXXXXX
```
![Image](https://github.com/user-attachments/assets/bb3c1eda-e8a3-4082-a8d4-1089eeb29c7c)

![Image](https://github.com/user-attachments/assets/4c71138c-5292-4c99-9617-78cdf515c81f)

![image](https://github.com/user-attachments/assets/985cc90b-7192-4341-a7b7-2fd28036ef4a)

![image](https://github.com/user-attachments/assets/7bead2cd-f15c-4c10-8dee-0162005f870d)

![image](https://github.com/user-attachments/assets/96189b5e-fa77-468f-becb-ce5af1f29a1b)

![image](https://github.com/user-attachments/assets/1e544f0d-4aab-454d-afc3-40caedf435a3)

![image](https://github.com/user-attachments/assets/8bc87e76-8730-4d87-8ec4-2713852f62a7)


*Capturas de logs de ejecuciÃ³n del pipeline completo*

---

## 9. âœ… ValidaciÃ³n y Pruebas

### Tests Automatizados

```bash
# Test de conexiones
python tests/test_connections.py

# ValidaciÃ³n completa del pipeline
python tests/validate_pipeline.py

# Tests especÃ­ficos de Spark
python spark_jobs/test_spark_jobs.py
```

### MÃ©tricas de ValidaciÃ³n

**[IMAGEN: validation_report.png]**
*Captura del reporte de validaciÃ³n del pipeline*

#### PuntuaciÃ³n de Ã‰xito
- **Checks totales**: 12
- **Checks exitosos**: 11
- **Tasa de Ã©xito**: 91.7%

#### ValidaciÃ³n por Componente

| Componente | Estado | Archivos/Registros |
|------------|--------|--------------------|
| S3 Raw Zone | âœ… OK | 15 archivos |
| S3 Trusted Zone | âœ… OK | 8 archivos |
| S3 Refined Zone | âœ… OK | 12 archivos |
| Athena Tables | âœ… OK | 4 tablas |
| Data Quality | âœ… OK | Estructura vÃ¡lida |

### Logs y Monitoreo

#### Ubicaciones de Logs
- **EMR Logs**: `s3://bucket-scripts/logs/emr/`
- **Lambda Logs**: CloudWatch `/aws/lambda/weather-api`
- **Application Logs**: CloudWatch custom log groups

---

## 10. ğŸŒ API y Consultas

### ConfiguraciÃ³n de Athena

```python
# CreaciÃ³n automÃ¡tica de tablas
python applications/setup_athena.py
```

![image](https://github.com/user-attachments/assets/5e97ac77-f732-4043-aa01-e2750a95c773)

*Captura de tablas creadas en Athena*

#### Tablas Disponibles

1. **integrated_weather_data**: Datos meteorolÃ³gicos integrados
2. **temperature_trends_monthly**: Tendencias mensuales por ciudad
3. **extreme_weather_events**: Eventos meteorolÃ³gicos extremos
4. **annual_weather_summary**: Resumen anual por ciudad

### Consultas SQL Disponibles

#### Temperatura Promedio por Ciudad
```sql
SELECT city_name, AVG(temp_avg) as avg_temp
FROM weather_analytics_db.integrated_weather_data
GROUP BY city_name
ORDER BY avg_temp DESC;
```

#### Eventos MeteorolÃ³gicos Extremos
```sql
SELECT city_name, event_type, SUM(event_count) as total_events
FROM weather_analytics_db.extreme_weather_events
GROUP BY city_name, event_type
ORDER BY total_events DESC;
```

#### Tendencias Mensuales de PrecipitaciÃ³n
```sql
SELECT city_name, month, avg_precipitation
FROM weather_analytics_db.temperature_trends_monthly
WHERE year = 2025
ORDER BY city_name, month;
```

### APIs REST Disponibles

#### ConfiguraciÃ³n de Lambda + API Gateway

```python
# FunciÃ³n Lambda para APIs
def lambda_handler(event, context):
    path = event.get('path', '/')
    
    if path == '/weather/current':
        return get_current_weather(query_params)
    elif path == '/weather/trends':
        return get_weather_trends(query_params)
    # ... mÃ¡s endpoints
```


#### Endpoints Disponibles

| Endpoint | MÃ©todo | DescripciÃ³n | ParÃ¡metros |
|----------|--------|-------------|------------|
| `/weather/current` | GET | Datos actuales | `city`, `limit` |
| `/weather/trends` | GET | Tendencias | `city`, `year` |
| `/weather/extreme-events` | GET | Eventos extremos | `event_type`, `year` |
| `/weather/summary` | GET | Resumen anual | `year`, `climate_category` |

#### Ejemplos de Uso

```bash
# Datos actuales de BogotÃ¡
GET /weather/current?city=bogota&limit=10

# Tendencias de MedellÃ­n en 2024
GET /weather/trends?city=medellin&year=2024

# Eventos de lluvia intensa
GET /weather/extreme-events?type=heavy_rain&year=2024
```

---

## 11. ğŸ“ˆ Monitoreo y Logs

### CloudWatch Integration


#### MÃ©tricas Monitoreadas

1. **EMR Cluster**
   - CPU utilization
   - Memory usage
   - HDFS usage
   - Jobs completed/failed

2. **S3 Storage**
   - Objects count
   - Storage size
   - Request metrics

3. **Lambda Functions**
   - Invocation count
   - Duration
   - Error rate

4. **Athena Queries**
   - Query execution time
   - Data scanned
   - Success rate

### Alertas Configuradas


```json
{
  "Alarms": [
    {
      "AlarmName": "EMR-HighCPUUtilization",
      "MetricName": "CPUUtilization",
      "Threshold": 80,
      "ComparisonOperator": "GreaterThanThreshold"
    },
    {
      "AlarmName": "Lambda-HighErrorRate", 
      "MetricName": "Errors",
      "Threshold": 5,
      "ComparisonOperator": "GreaterThanThreshold"
    }
  ]
}
```

### Logs Estructurados

#### Formato de Logs
```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger.info("âœ… Data uploaded", extra={
    "bucket": bucket_name,
    "key": s3_key,
    "records": record_count,
    "city": city_name
})
```


---
## 12. VisualizaciÃ³n con Colab
A continuaciÃ³n mostraremos diferentes grÃ¡ficas estadÃ­sticas y de analÃ­tica sobre algunos de los archivos que obtuvimos desde la API de Open-Meteo.

**Al ejecutar:**

```python
python visualizations/export_data_for_colab.py
```

Este nos genera un archivo .csv obtenido desde los archivos .json que se adjuntaron al S3 de AWS.

Luego en Colab con diferentes celdas pudimos obtener las siguientes grÃ¡ficas informativas y descriptivas:

```python
# Importar las librerias 
!pip install plotly pandas seaborn matplotlib folium -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json

print("ğŸŒ¤ï¸ WEATHER ANALYTICS - ANÃLISIS COMPLETO")
print("="*60)
```
### **Pruebas de consultas:**

```python
print("ğŸ“Š ANÃLISIS EXPLORATORIO")
print(df.describe())
print(f"\nğŸ™ï¸ Ciudades analizadas: {df['city_name'].nunique()}")
print(df['city_name'].value_counts())
```
![image](https://github.com/user-attachments/assets/e81e8521-e890-4a80-8c95-a02e4f1ffa85)

### Diagramas de Tendencias

```python
fig = px.line(df,
              x='date',
              y='temp_avg',
              color='city_name',
              title='ğŸŒ¡ï¸ Tendencias de Temperatura - Ciudades Colombianas',
              markers=True,
              hover_data=['temp_max', 'temp_min'])

fig.update_layout(
    xaxis_title="Fecha",
    yaxis_title="Temperatura Promedio (Â°C)",
    hovermode='x unified',
    height=500
)

fig.show()
```
![image](https://github.com/user-attachments/assets/90702ede-d8b1-473a-8a67-65e3d67652ef)

### Mapa de Temperaturas en Colombia:
```python
city_coords = {
    'BogotÃ¡': {'lat': 4.6097, 'lon': -74.0817},
    'MedellÃ­n': {'lat': 6.2518, 'lon': -75.5636},
    'Cali': {'lat': 3.4516, 'lon': -76.5320},
    'Cartagena': {'lat': 10.3910, 'lon': -75.4794},
    'Barranquilla': {'lat': 10.9639, 'lon': -74.7964}
}

# Agregar coordenadas al DataFrame
for city, coords in city_coords.items():
    mask = df['city_name'] == city
    df.loc[mask, 'lat'] = coords['lat']
    df.loc[mask, 'lon'] = coords['lon']

# Calcular temperatura promedio por ciudad
city_temps = df.groupby('city_name').agg({
    'temp_avg': 'mean',
    'lat': 'first',
    'lon': 'first'
}).reset_index()

# Mapa interactivo
fig = px.scatter_mapbox(city_temps,
                        lat='lat',
                        lon='lon',
                        size='temp_avg',
                        color='temp_avg',
                        hover_name='city_name',
                        hover_data={'temp_avg': ':.1fÂ°C'},
                        color_continuous_scale='RdYlBu_r',
                        title='ğŸ—ºï¸ Mapa de Temperaturas - Colombia',
                        zoom=5)

fig.update_layout(mapbox_style="open-street-map",
                  mapbox_center_lat=5.5,
                  mapbox_center_lon=-75,
                  height=600)
fig.show()
```

![image](https://github.com/user-attachments/assets/dd7df996-f2b9-405e-8546-f3de91fc54ea)

```python
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=('ğŸ“ˆ Temperatura por Ciudad',
                   'ğŸŒ§ï¸ PrecipitaciÃ³n vs Temperatura',
                   'ğŸ“Š DistribuciÃ³n Temperaturas',
                   'ğŸ“ˆ Tendencia Semanal'),
    specs=[[{"secondary_y": False}, {"secondary_y": False}],
           [{"secondary_y": False}, {"secondary_y": False}]]
)

# GrÃ¡fico 1: Box plot temperaturas
for city in df['city_name'].unique():
    city_data = df[df['city_name'] == city]
    fig.add_trace(
        go.Box(y=city_data['temp_avg'], name=city, showlegend=False),
        row=1, col=1
    )

# GrÃ¡fico 2: Scatter precipitaciÃ³n vs temperatura
fig.add_trace(
    go.Scatter(x=df['temp_avg'], y=df['precipitation'],
               mode='markers', text=df['city_name'],
               name='Temp vs Precip', showlegend=False),
    row=1, col=2
)

# GrÃ¡fico 3: Histograma temperaturas
fig.add_trace(
    go.Histogram(x=df['temp_avg'], name='DistribuciÃ³n', showlegend=False),
    row=2, col=1
)

# GrÃ¡fico 4: Promedio diario
daily_avg = df.groupby('date')['temp_avg'].mean().reset_index()
fig.add_trace(
    go.Scatter(x=daily_avg['date'], y=daily_avg['temp_avg'],
               mode='lines+markers', name='Promedio diario', showlegend=False),
    row=2, col=2
)

fig.update_layout(height=800, title_text="ğŸ“Š Dashboard Weather Analytics Colombia")
fig.show()
```
### GrÃ¡ficas de cajas, barras, dispersiÃ³n y tendencia

![image](https://github.com/user-attachments/assets/163a050c-0ba6-4cbb-9d09-da0e3f5c9e66)

## 13. ğŸ¯ Conclusiones

### Logros Alcanzados

1. **âœ… AutomatizaciÃ³n Completa**
   - Pipeline end-to-end sin intervenciÃ³n manual
   - Ingesta automatizada desde mÃºltiples fuentes
   - Procesamiento ETL con Spark en EMR
   - AnÃ¡lisis descriptivo y predictivo

2. **âœ… Escalabilidad y Performance**
   - Arquitectura serverless moderna
   - Auto-scaling en EMR
   - Almacenamiento optimizado en S3
   - Consultas rÃ¡pidas con Athena

3. **âœ… Calidad de Datos**
   - ValidaciÃ³n automÃ¡tica de datos
   - Limpieza y estandarizaciÃ³n
   - IntegraciÃ³n de mÃºltiples fuentes
   - DetecciÃ³n de anomalÃ­as

4. **âœ… Accesibilidad**
   - APIs REST documentadas
   - Consultas SQL ad-hoc
   - Resultados en formatos estÃ¡ndar
   - DocumentaciÃ³n completa


### Arquitectura Implementada

La soluciÃ³n implementa exitosamente una **arquitectura Data Lake moderna** con:

- **Zona Raw**: Datos crudos sin procesar
- **Zona Trusted**: Datos limpios y validados  
- **Zona Refined**: Resultados analÃ­ticos y modelos

Cumpliendo con todos los requerimientos del Trabajo 3:
- âœ… Ingesta automatizada
- âœ… Procesamiento ETL con Spark
- âœ… AnÃ¡lisis descriptivo y predictivo
- âœ… Acceso via Athena y APIs
- âœ… Pipeline completamente automatizado

### TecnologÃ­as Clave

- **AWS S3**: Storage escalable y durÃ¡vel
- **Amazon EMR**: Procesamiento Spark managed
- **Amazon Athena**: Query engine serverless
- **AWS Lambda**: APIs serverless
- **Apache Spark**: Motor de Big Data
- **Python**: Lenguaje de implementaciÃ³n

### Impacto y Valor

Este proyecto demuestra la implementaciÃ³n de una soluciÃ³n completa de **ingenierÃ­a de datos big data** que puede:

1. **Escalar** a millones de registros meteorolÃ³gicos
2. **Procesar** datos en tiempo real y batch
3. **Predecir** tendencias climÃ¡ticas futuras
4. **Detectar** eventos meteorolÃ³gicos extremos
5. **Servir** datos via APIs para aplicaciones

### PrÃ³ximos Pasos

1. **ExtensiÃ³n de Fuentes de Datos**
   - IntegraciÃ³n con mÃ¡s APIs meteorolÃ³gicas
   - Datos de satÃ©lites y radares
   - Sensores IoT en tiempo real

2. **Mejoras en ML**
   - Modelos mÃ¡s sofisticados (Deep Learning)
   - Predicciones a largo plazo
   - DetecciÃ³n automÃ¡tica de patrones

3. **AutomatizaciÃ³n Avanzada**
   - Scheduling con EventBridge
   - OrquestaciÃ³n con Step Functions
   - CI/CD pipelines

4. **VisualizaciÃ³n**
   - Dashboards interactivos con QuickSight
   - Mapas de calor geogrÃ¡ficos
   - Alertas en tiempo real

### Lecciones Aprendidas

1. **Arquitectura Serverless**
   - MÃ¡s eficiente que instancias EC2 dedicadas
   - Auto-scaling automÃ¡tico
   - Costos optimizados para cargas variables

2. **Data Lake vs Data Warehouse**
   - Flexibilidad para datos no estructurados
   - Schema-on-read vs schema-on-write
   - Mejor para casos de uso exploratorios

3. **Apache Spark en EMR**
   - Potente para procesamiento distribuido
   - Requiere optimizaciÃ³n de recursos
   - Ideal para transformaciones complejas

---

## 14. ğŸ“š Referencias y Recursos

### DocumentaciÃ³n TÃ©cnica

- [AWS EMR Developer Guide](https://docs.aws.amazon.com/emr/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [AWS S3 User Guide](https://docs.aws.amazon.com/s3/)
- [Amazon Athena User Guide](https://docs.aws.amazon.com/athena/)

### APIs Utilizadas

- [OpenMeteo API Documentation](https://open-meteo.com/en/docs)
- [AWS SDK for Python (Boto3)](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)

### Herramientas de Desarrollo

- **IDE**: Visual Studio Code
- **Control de Versiones**: Git
- **Testing**: pytest, moto
- **Documentation**: Markdown

---

## ğŸ“ Anexos

### Anexo A: ConfiguraciÃ³n Detallada de AWS

#### A.1 PolÃ­ticas IAM Requeridas

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::weather-analytics-*",
                "arn:aws:s3:::weather-analytics-*/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "emr:RunJobFlow",
                "emr:DescribeCluster",
                "emr:AddJobFlowSteps",
                "emr:DescribeStep"
            ],
            "Resource": "*"
        }
    ]
}
```

#### A.2 ConfiguraciÃ³n de VPC para RDS

```yaml
VPC:
  Type: AWS::EC2::VPC
  Properties:
    CidrBlock: 10.0.0.0/16
    EnableDnsHostnames: true
    EnableDnsSupport: true

PrivateSubnet1:
  Type: AWS::EC2::Subnet
  Properties:
    VpcId: !Ref VPC
    CidrBlock: 10.0.1.0/24
    AvailabilityZone: us-east-1a

PrivateSubnet2:
  Type: AWS::EC2::Subnet
  Properties:
    VpcId: !Ref VPC
    CidrBlock: 10.0.2.0/24
    AvailabilityZone: us-east-1b
```

### Anexo B: Ejemplos de Datos

#### B.1 Estructura JSON de Datos API

```json
{
  "city_info": {
    "name": "BogotÃ¡",
    "lat": 4.6097,
    "lon": -74.0817
  },
  "daily": {
    "time": ["2025-05-30", "2025-05-31"],
    "temperature_2m_max": [22.5, 24.1],
    "temperature_2m_min": [12.3, 14.2],
    "precipitation_sum": [2.5, 0.0],
    "windspeed_10m_max": [15.2, 18.7],
    "relative_humidity_2m_mean": [75.5, 68.3]
  },
  "ingestion_timestamp": "2025-05-30T14:30:00Z",
  "data_source": "openmeteo_api"
}
```

#### B.2 Schema Parquet de Datos Procesados

```
root
 |-- city_name: string (nullable = true)
 |-- latitude: double (nullable = true)
 |-- longitude: double (nullable = true)
 |-- date: date (nullable = true)
 |-- temp_max: double (nullable = true)
 |-- temp_min: double (nullable = true)
 |-- temp_avg: double (nullable = true)
 |-- precipitation: double (nullable = true)
 |-- wind_speed: double (nullable = true)
 |-- humidity: double (nullable = true)
 |-- year: integer (nullable = true)
 |-- month: integer (nullable = true)
```

### Anexo C: Scripts de Utilidad

#### C.1 Script de Limpieza

```bash
#!/bin/bash
# cleanup_resources.sh

echo "ğŸ§¹ Limpiando recursos AWS..."

# Terminar clÃºster EMR
aws emr terminate-clusters --cluster-ids $(aws emr list-clusters --active --query 'Clusters[0].Id' --output text)

# Vaciar buckets S3
aws s3 rm s3://weather-analytics-pipeline-raw-* --recursive
aws s3 rm s3://weather-analytics-pipeline-trusted-* --recursive
aws s3 rm s3://weather-analytics-pipeline-refined-* --recursive
aws s3 rm s3://weather-analytics-pipeline-scripts-* --recursive

# Eliminar buckets
aws s3 rb s3://weather-analytics-pipeline-raw-* --force
aws s3 rb s3://weather-analytics-pipeline-trusted-* --force
aws s3 rb s3://weather-analytics-pipeline-refined-* --force
aws s3 rb s3://weather-analytics-pipeline-scripts-* --force

echo "âœ… Limpieza completada"
```

#### C.2 Script de Backup

```python
#!/usr/bin/env python3
# backup_project.py

import boto3
import datetime
import json

def backup_s3_data():
    """Crear backup de datos importantes"""
    s3 = boto3.client('s3')
    
    # Cargar configuraciÃ³n
    with open('config/buckets.json', 'r') as f:
        buckets = json.load(f)
    
    backup_bucket = f"weather-analytics-backup-{datetime.datetime.now().strftime('%Y%m%d')}"
    
    # Crear bucket de backup
    s3.create_bucket(Bucket=backup_bucket)
    
    # Copiar datos crÃ­ticos
    for bucket_type, bucket_name in buckets.items():
        if bucket_type in ['trusted', 'refined']:
            # CÃ³digo de copia...
            pass
    
    print(f"âœ… Backup completado en: {backup_bucket}")

if __name__ == "__main__":
    backup_s3_data()



---

*Este proyecto implementa una soluciÃ³n completa de Big Data para anÃ¡lisis meteorolÃ³gico, cumpliendo con todos los requerimientos tÃ©cnicos y demostrando competencias avanzadas en ingenierÃ­a de datos, cloud computing y machine learning.*
