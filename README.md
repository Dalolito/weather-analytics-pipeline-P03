# TÃ³picos Especiales en TelemÃ¡tica
## Proyecto 03, Proyecto de Weather Analytics con Big Data 

## Estudiante(s):
| Nombre | Correo |
|--------|-------------|
| David Lopera LondoÃ±o | dloperal2@eafit.edu.co |
| Camilo Monsalve Montes | cmonsalvem@eafit.edu.co |
| Juan Diego AcuÃ±a Giraldo | jdacunag@eafit.edu.co |

## Profesor:
| Profesor | Correo |
|----------|-------------|
| Edwin Nelson Montoya MÃºnera | emontoya@eafit.edu.co |

## VÃ­deo de la SustentaciÃ³n

## 1. DescripciÃ³n de la actividad
Este proyecto implementa una arquitectura batch completa de Big Data para anÃ¡lisis meteorolÃ³gico usando tecnologÃ­as AWS y Apache Spark. El sistema automatiza el proceso completo desde la captura de datos hasta los modelos predictivos y consultas SQL.
### Objetivos:

âœ… Captura automÃ¡tica de datos meteorolÃ³gicos desde API OpenMeteo y base de datos MySQL

âœ… Ingesta automatizada hacia buckets S3 organizados por zonas (Raw, Trusted, Refined)

âœ… Procesamiento ETL con Apache Spark en clÃºster EMR

âœ… AnÃ¡lisis descriptivo y modelos de machine learning

âœ… Consultas SQL mediante Amazon Athena

âœ… API REST para acceso a resultados

### ProblemÃ¡tica analizada:
AnÃ¡lisis y predicciÃ³n de patrones meteorolÃ³gicos en ciudades colombianas para:

* Identificar tendencias climÃ¡ticas por ciudad y temporada
* Detectar eventos meteorolÃ³gicos extremos
* Generar pronÃ³sticos basados en datos histÃ³ricos
* Proporcionar datos accionables para la toma de decisiones

## 1.2 Aspectos NO cumplidos o desarrollados
Tuvimos limitaciones con:

* **AWS Academy:** Permisos limitados para algunos servicios avanzados
* **API Gateway:** ConfiguraciÃ³n manual requerida (instrucciones proporcionadas)
* **Machine Learning:** ImplementaciÃ³n simplificada debido a restricciones de librerÃ­as en EMR
  
## 2. InformaciÃ³n general del proyecto


### ðŸ“Š Fuente de Datos: Open-Meteo

* **API Principal:** https://api.open-meteo.com/v1/forecast
* **Datos HistÃ³ricos:** https://archive-api.open-meteo.com/v1/archive
* **Ciudades analizadas:** BogotÃ¡, MedellÃ­n, Cali, Cartagena, Barranquilla

**Variables meteorolÃ³gicas:**

* Temperatura (mÃ¡xima, mÃ­nima, promedio)
* PrecipitaciÃ³n
* Humedad relativa
* Velocidad del viento
* PresiÃ³n atmosfÃ©rica


### Base de Datos MySQL (RDS)

Estaciones meteorolÃ³gicas: Metadatos de ubicaciones
Eventos climÃ¡ticos histÃ³ricos: Registro de eventos extremos
Umbrales de alerta: Configuraciones para detecciÃ³n de anomalÃ­as

### Arquitectura del Sistema

- **Raw Zone**: Datos crudos de APIs y base de datos
- **Trusted Zone**: Datos procesados y limpios
- **Refined Zone**: Datos analÃ­ticos y modelos ML

### Flujo del Proyecto 

**1. Captura:** APIs y BD â†’ JSON crudo

**2. Ingesta:** Almacenamiento automÃ¡tico en S3 Raw
   
**3.  ETL:** Spark procesa y limpia â†’ Parquet en S3 Trusted
   
**4. Analytics:** AnÃ¡lisis estadÃ­stico â†’ Parquet en S3 Refined
   
**5. ML:** Modelos predictivos simples â†’ MÃ©tricas y pronÃ³sticos

**6. Consultas:** Athena + API para acceso final

### Diagrama de Arquitectura del Sistema

![Image](https://github.com/user-attachments/assets/84b1592f-a493-4401-84bf-7565e5576d01)

## Â¿QuÃ© implementamos?

### Infraestructura base:

* 4 buckets S3 organizados por zonas de datos
* ClÃºster EMR con Apache Spark (Master + Worker)
* Base de datos RDS MySQL con datos simulados
* Amazon Athena configurado con 7 tablas externas
* Roles y permisos IAM para integraciÃ³n de servicios

### Componentes de software:

* Scripts de ingesta automÃ¡tica desde mÃºltiples fuentes
* Jobs de Spark para ETL, analytics y ML
* Orquestador para coordinaciÃ³n del pipeline
* API Lambda para consultas REST
* Scripts de configuraciÃ³n de infraestructura AWS
* Suite de validaciÃ³n y testing automatizado

### Funcionalidades de la aplicaciÃ³n:

* Pipeline completamente automatizado de datos meteorolÃ³gicos
* AnÃ¡lisis de tendencias mensuales por ciudad
* DetecciÃ³n de eventos extremos (olas de calor, lluvias intensas)
* PronÃ³sticos bÃ¡sicos basados en promedios histÃ³ricos
* Consultas SQL interactivas via Athena
* API REST con endpoints para diferentes tipos de anÃ¡lisis
* VisualizaciÃ³n con grÃ¡ficas en Google Colab con los metadatos extraÃ­dos desde OpenMeteo

### CaracterÃ­sticas tÃ©cnicas:

* Procesamiento distribuido con Apache Spark
* Almacenamiento columnar optimizado (Parquet)
* Particionado inteligente por aÃ±o/mes
* Manejo robusto de errores y recuperaciÃ³n
* Escalabilidad horizontal en EMR
* CompresiÃ³n automÃ¡tica de datos

### Estructura del proyecto

```
Bookstore-P02/
â”œâ”€â”€ applications/ 
â”‚   â”œâ”€â”€ setup_athena.py                  
â”‚   â””â”€â”€ weaher_api.py
â”œâ”€â”€ automation/ 
â”‚   â””â”€â”€ orchestrator.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ bucket.json  
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ setup_database.py
â”‚   â”œâ”€â”€ setup_emr_cluster.py
â”‚   â”œâ”€â”€ setup_emr_cluster_improved.py
â”‚   â””â”€â”€setup_e3_buckets.py  
â”œâ”€â”€ ingesta/
â”‚   â”œâ”€â”€ fetch_database.py
â”‚   â””â”€â”€ fetch_openmeteo_api.py  
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ analythics_weather_trends.py
â”‚   â”œâ”€â”€ etl_weather_data.py
â”‚   â”œâ”€â”€ ml_weather_prediction.py  
â”‚   â””â”€â”€ test_spark_jobs.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ trusted/
â”‚   â”œâ”€â”€ refined/ 
â”‚   â””â”€â”€ scripts/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_aws_connection.py
â”‚   â”œâ”€â”€ tests_connections.py
â”‚   â”œâ”€â”€ validate_pipeline.py  
â”‚   â””â”€â”€ validate_pipeline_improved.py
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ notebooks/
â”‚         â””â”€â”€ Colab1/  
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_project.py
â”œâ”€â”€ verify_project_setup.py
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

## 3. DescripciÃ³n del ambiente de desarrollo y tÃ©cnico

### Lenguajes, librerÃ­as y tecnologÃ­as usadas

**Lenguajes de programaciÃ³n:**
|Lenguaje|JustificaciÃ³n|
|--|--|
| Python 3.8+: | Lenguaje principal del proyecto |
|SQL:| Para consultas en Athena y configuraciÃ³n de BD|
|JSON:| Formato de intercambio de datos|

**LibrerÃ­as principales:**
|python| Datos y anÃ¡lisis|
|--|--|
|pandas| ManipulaciÃ³n de datos|
|pyspark |Procesamiento distribuido con Spark|

| AWS SDK | ExpliaciÃ³n  |
|--|--|
|boto3 |  SDK de AWS para Python|
|requests|HTTP requests para APIs|

**Base de datos:**
* mysql-connector-python: Para conexiÃ³n a MySQL/RDS

**Utilidades**:
| LibrerÃ­a | ExplicaciÃ³n |
|--|--|
|python-dotenv |Manejo de variables de entorno|
|pytest    |Testing framework|
|jupyter  | Notebooks (desarrollo)|

**Herramientas de AWS:**
| Herramienta | ExplicaciÃ³n  
|--|--|
|Amazon S3:| Data Lake con 4 buckets organizados por zonas|
|Amazon EMR:| ClÃºster Apache Spark para procesamiento distribuido|
|Amazon RDS MySQL:| Base de datos relacional|
|Amazon Athena:| Motor SQL para consultas sobre datos procesados|
|AWS Lambda:| Functions para API REST|
|AWS IAM:| GestiÃ³n de permisos y roles|


### CompilaciÃ³n y ejecuciÃ³n

**Clonar repositorio:**

```bash
git clone https://github.com/Dalolito/weather-analytics-pipeline-P03.git
cd weather-analytics-pipeline-P03
```

**Crear entorno virtual:**

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows
```

**Instalar dependencias:**

```bash
pip install -r requirements.txt
```

**Configurar credenciales AWS:**

```bash
# Crear archivo .env
AWS_DEFAULT_REGION=us-east-1
AWS_ACCESS_KEY_ID=tu_access_key
AWS_SECRET_ACCESS_KEY=tu_secret_key
AWS_SESSION_TOKEN=tu_session_token

# Database 
DB_HOST=tu-rds-endpoint.us-east-1.rds.amazonaws.com
DB_USER=admin
DB_PASSWORD=tu_password
DB_NAME=weather_data
```

**EjecuciÃ³n del proyecto:**

```bash
# 1. Verificar configuraciÃ³n completa
python verify_project_setup.py
```

```bash
# 2. Configurar infraestructura AWS
python infrastructure/setup_s3_buckets.py
python infrastructure/setup_emr_cluster_improved.py
```

```bash
# 3. Ejecutar pipeline completo
python run_project.py --step full-pipeline --cluster-id CLUSTER_ID
```

```bash
# 4. Configurar Athena para consultas
python run_project.py --step setup-athena
```

```bash
# 5. Validar resultados
python tests/validate_pipeline_improved.py
```

## 4. DescripciÃ³n del ambiente de EJECUCIÃ“N (en producciÃ³n)

## Quick Start

1. Configurar AWS Academy credentials
2. Ejecutar `python infrastructure/setup_s3_buckets.py`
3. Configurar RDS MySQL
4. Ejecutar `python infrastructure/setup_database.py`
5. Ejecutar `python tests/test_connections.py`
6. Ejecutar `python ingesta/fetch_openmeteo_api.py`

## Referencias
